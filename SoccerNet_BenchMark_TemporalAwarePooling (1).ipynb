{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SoccerNet_BenchMark_TemporalAwarePooling.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Installing SoccerNet"
      ],
      "metadata": {
        "id": "quzdXyWizNbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6x1qbC1U3JrA",
        "outputId": "6e4076da-0e24-4d0a-b330-d9f121aa761b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install SoccerNet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KNpPdg5A2Ez",
        "outputId": "cb23f5ac-5841-4120-ca12-016fbabda7d3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting SoccerNet\n",
            "  Downloading SoccerNet-0.1.33-py2.py3-none-any.whl (68 kB)\n",
            "\u001b[?25l\r\u001b[K     |████▉                           | 10 kB 19.7 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 20 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 30 kB 10.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 40 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 51 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 61 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 68 kB 3.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from SoccerNet) (3.2.2)\n",
            "Collecting scikit-video\n",
            "  Downloading scikit_video-1.1.11-py2.py3-none-any.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 8.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from SoccerNet) (4.64.0)\n",
            "Collecting google-measurement-protocol\n",
            "  Downloading google_measurement_protocol-1.1.0-py2.py3-none-any.whl (5.9 kB)\n",
            "Requirement already satisfied: requests<3.0a0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from google-measurement-protocol->SoccerNet) (2.23.0)\n",
            "Collecting prices>=1.0.0\n",
            "  Downloading prices-1.1.0-py2.py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: babel>=2.5.0 in /usr/local/lib/python3.7/dist-packages (from prices>=1.0.0->google-measurement-protocol->SoccerNet) (2.10.1)\n",
            "Requirement already satisfied: pytz>=2015.7 in /usr/local/lib/python3.7/dist-packages (from babel>=2.5.0->prices>=1.0.0->google-measurement-protocol->SoccerNet) (2022.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0a0,>=2.0->google-measurement-protocol->SoccerNet) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0a0,>=2.0->google-measurement-protocol->SoccerNet) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0a0,>=2.0->google-measurement-protocol->SoccerNet) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0a0,>=2.0->google-measurement-protocol->SoccerNet) (3.0.4)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->SoccerNet) (1.4.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->SoccerNet) (3.0.8)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib->SoccerNet) (1.21.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->SoccerNet) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->SoccerNet) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->SoccerNet) (4.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->SoccerNet) (1.15.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from scikit-video->SoccerNet) (7.1.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from scikit-video->SoccerNet) (1.4.1)\n",
            "Installing collected packages: prices, scikit-video, google-measurement-protocol, SoccerNet\n",
            "Successfully installed SoccerNet-0.1.33 google-measurement-protocol-1.1.0 prices-1.1.0 scikit-video-1.1.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "524lE6sazR7C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import logging\n",
        "import os\n",
        "import zipfile\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "y4JLCkbSzFAf"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting and Preprocessing Dataset"
      ],
      "metadata": {
        "id": "2U7m8QjszUts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import time\n",
        "\n",
        "import sklearn\n",
        "import sklearn.metrics\n",
        "from sklearn.metrics import average_precision_score\n",
        "from SoccerNet.Evaluation.ActionSpotting import evaluate\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "\n",
        "import logging\n",
        "import json\n",
        "\n",
        "from SoccerNet.Downloader import getListGames\n",
        "from SoccerNet.Downloader import SoccerNetDownloader\n",
        "from SoccerNet.Evaluation.utils import AverageMeter, EVENT_DICTIONARY_V2, INVERSE_EVENT_DICTIONARY_V2\n",
        "from SoccerNet.Evaluation.utils import EVENT_DICTIONARY_V1, INVERSE_EVENT_DICTIONARY_V1\n",
        "\n",
        "\n",
        "\n",
        "def feats2clip(feats, stride, clip_length, padding = \"replicate_last\", off=0):\n",
        "    if padding ==\"zeropad\":\n",
        "        print(\"beforepadding\", feats.shape)\n",
        "        pad = feats.shape[0] - int(feats.shape[0]/stride)*stride\n",
        "        print(\"pad need to be\", clip_length-pad)\n",
        "        m = torch.nn.ZeroPad2d((0, 0, clip_length-pad, 0))\n",
        "        feats = m(feats)\n",
        "        print(\"afterpadding\", feats.shape)\n",
        "        # nn.ZeroPad2d(2)\n",
        "\n",
        "    idx = torch.arange(start=0, end=feats.shape[0]-1, step=stride)\n",
        "    idxs = []\n",
        "    for i in torch.arange(-off, clip_length-off):\n",
        "        idxs.append(idx+i)\n",
        "    idx = torch.stack(idxs, dim=1)\n",
        "\n",
        "    if padding==\"replicate_last\":\n",
        "        idx = idx.clamp(0, feats.shape[0]-1)\n",
        "    # print(idx)\n",
        "    return feats[idx,...]\n",
        "\n",
        "\n",
        "class SoccerNetClips(Dataset):\n",
        "    def __init__(self, path, features=\"baidu_soccer_embeddings.npy\", split=[\"train\"], version=2, \n",
        "                framerate=1, window_size=10):\n",
        "        self.path = path\n",
        "        self.listGames = getListGames(split)[:50]\n",
        "        self.features = features\n",
        "        self.window_size_frame = window_size*framerate\n",
        "        self.version = version\n",
        "        if version == 1:\n",
        "            self.num_classes = 3\n",
        "            self.labels=\"Labels.json\"\n",
        "        elif version == 2:\n",
        "            self.dict_event = EVENT_DICTIONARY_V2\n",
        "            self.num_classes = 17\n",
        "            self.labels=\"Labels-v2.json\"\n",
        "\n",
        "        logging.info(\"Checking/Download features and labels locally\")\n",
        "        downloader = SoccerNetDownloader(path)\n",
        "        downloader.downloadGames(files=[self.labels, f\"1_{self.features}\", f\"2_{self.features}\"], split=split, verbose=False,randomized=True)\n",
        "\n",
        "\n",
        "        logging.info(\"Pre-compute clips\")\n",
        "\n",
        "        self.game_feats = list()\n",
        "        self.game_labels = list()\n",
        "\n",
        "        # game_counter = 0\n",
        "        \n",
        "        for game in tqdm(self.listGames):\n",
        "            # Load features\n",
        "            feat_half1 = np.load(os.path.join(self.path, game, \"1_\" + self.features))\n",
        "            feat_half1 = feat_half1.reshape(-1, feat_half1.shape[-1])\n",
        "            feat_half2 = np.load(os.path.join(self.path, game, \"2_\" + self.features))\n",
        "            feat_half2 = feat_half2.reshape(-1, feat_half2.shape[-1])\n",
        "\n",
        "            feat_half1 = feats2clip(torch.from_numpy(feat_half1), stride=self.window_size_frame, clip_length=self.window_size_frame)\n",
        "            feat_half2 = feats2clip(torch.from_numpy(feat_half2), stride=self.window_size_frame, clip_length=self.window_size_frame)\n",
        "\n",
        "            # Load labels\n",
        "            labels = json.load(open(os.path.join(self.path, game, self.labels)))\n",
        "\n",
        "            label_half1 = np.zeros((feat_half1.shape[0], self.num_classes+1))\n",
        "            label_half1[:,0]=1 # those are BG classes\n",
        "            label_half2 = np.zeros((feat_half2.shape[0], self.num_classes+1))\n",
        "            label_half2[:,0]=1 # those are BG classes\n",
        "\n",
        "\n",
        "            for annotation in labels[\"annotations\"]:\n",
        "\n",
        "                time = annotation[\"gameTime\"]\n",
        "                event = annotation[\"label\"]\n",
        "\n",
        "                half = int(time[0])\n",
        "\n",
        "                minutes = int(time[-5:-3])\n",
        "                seconds = int(time[-2::])\n",
        "                frame = framerate * ( seconds + 60 * minutes ) \n",
        "\n",
        "                if version == 1:\n",
        "                    if \"card\" in event: label = 0\n",
        "                    elif \"subs\" in event: label = 1\n",
        "                    elif \"soccer\" in event: label = 2\n",
        "                    else: continue\n",
        "                elif version == 2:\n",
        "                    if event not in self.dict_event:\n",
        "                        continue\n",
        "                    label = self.dict_event[event]\n",
        "\n",
        "                # if label outside temporal of view\n",
        "                if half == 1 and frame//self.window_size_frame>=label_half1.shape[0]:\n",
        "                    continue\n",
        "                if half == 2 and frame//self.window_size_frame>=label_half2.shape[0]:\n",
        "                    continue\n",
        "\n",
        "                if half == 1:\n",
        "                    label_half1[frame//self.window_size_frame][0] = 0 # not BG anymore\n",
        "                    label_half1[frame//self.window_size_frame][label+1] = 1 # that's my class\n",
        "\n",
        "                if half == 2:\n",
        "                    label_half2[frame//self.window_size_frame][0] = 0 # not BG anymore\n",
        "                    label_half2[frame//self.window_size_frame][label+1] = 1 # that's my class\n",
        "            \n",
        "            self.game_feats.append(feat_half1)\n",
        "            self.game_feats.append(feat_half2)\n",
        "            self.game_labels.append(label_half1)\n",
        "            self.game_labels.append(label_half2)\n",
        "\n",
        "        self.game_feats = np.concatenate(self.game_feats)\n",
        "        self.game_labels = np.concatenate(self.game_labels)\n",
        "\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            index (int): Index\n",
        "        Returns:\n",
        "            clip_feat (np.array): clip of features.\n",
        "            clip_labels (np.array): clip of labels for the segmentation.\n",
        "            clip_targets (np.array): clip of targets for the spotting.\n",
        "        \"\"\"\n",
        "        return self.game_feats[index,:,:], self.game_labels[index,:]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.game_feats)\n",
        "\n",
        "\n",
        "class SoccerNetClipsTesting(Dataset):\n",
        "    def __init__(self, path, features=\"baidu_soccer_embeddings.npy\", split=[\"test\"], version=2, \n",
        "                framerate=1, window_size=10):\n",
        "        self.path = path\n",
        "        self.listGames = getListGames(split)\n",
        "        self.features = features\n",
        "        self.window_size_frame = window_size*framerate\n",
        "        self.framerate = framerate\n",
        "        self.version = version\n",
        "        self.split=split\n",
        "        if version == 1:\n",
        "            self.dict_event = EVENT_DICTIONARY_V1\n",
        "            self.num_classes = 3\n",
        "            self.labels=\"Labels.json\"\n",
        "        elif version == 2:\n",
        "            self.dict_event = EVENT_DICTIONARY_V2\n",
        "            self.num_classes = 17\n",
        "            self.labels=\"Labels-v2.json\"\n",
        "\n",
        "        logging.info(\"Checking/Download features and labels locally\")\n",
        "        downloader = SoccerNetDownloader(path)\n",
        "        for s in split:\n",
        "            if s == \"challenge\":\n",
        "                downloader.downloadGames(files=[f\"1_{self.features}\", f\"2_{self.features}\"], split=[s], verbose=False,randomized=True)\n",
        "            else:\n",
        "                downloader.downloadGames(files=[self.labels, f\"1_{self.features}\", f\"2_{self.features}\"], split=[s], verbose=False,randomized=True)\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            index (int): Index\n",
        "        Returns:\n",
        "            feat_half1 (np.array): features for the 1st half.\n",
        "            feat_half2 (np.array): features for the 2nd half.\n",
        "            label_half1 (np.array): labels (one-hot) for the 1st half.\n",
        "            label_half2 (np.array): labels (one-hot) for the 2nd half.\n",
        "        \"\"\"\n",
        "        # Load features\n",
        "        feat_half1 = np.load(os.path.join(self.path, self.listGames[index], \"1_\" + self.features))\n",
        "        feat_half1 = feat_half1.reshape(-1, feat_half1.shape[-1])\n",
        "        feat_half2 = np.load(os.path.join(self.path, self.listGames[index], \"2_\" + self.features))\n",
        "        feat_half2 = feat_half2.reshape(-1, feat_half2.shape[-1])\n",
        "\n",
        "        # Load labels\n",
        "        label_half1 = np.zeros((feat_half1.shape[0], self.num_classes))\n",
        "        label_half2 = np.zeros((feat_half2.shape[0], self.num_classes))\n",
        "\n",
        "        # check if annoation exists\n",
        "        if os.path.exists(os.path.join(self.path, self.listGames[index], self.labels)):\n",
        "            labels = json.load(open(os.path.join(self.path, self.listGames[index], self.labels)))\n",
        "\n",
        "            for annotation in labels[\"annotations\"]:\n",
        "\n",
        "                time = annotation[\"gameTime\"]\n",
        "                event = annotation[\"label\"]\n",
        "\n",
        "                half = int(time[0])\n",
        "\n",
        "                minutes = int(time[-5:-3])\n",
        "                seconds = int(time[-2::])\n",
        "                frame = self.framerate * ( seconds + 60 * minutes ) \n",
        "\n",
        "                if self.version == 1:\n",
        "                    if \"card\" in event: label = 0\n",
        "                    elif \"subs\" in event: label = 1\n",
        "                    elif \"soccer\" in event: label = 2\n",
        "                    else: continue\n",
        "                elif self.version == 2:\n",
        "                    if event not in self.dict_event:\n",
        "                        continue\n",
        "                    label = self.dict_event[event]\n",
        "\n",
        "                value = 1\n",
        "                if \"visibility\" in annotation.keys():\n",
        "                    if annotation[\"visibility\"] == \"not shown\":\n",
        "                        value = -1\n",
        "\n",
        "                if half == 1:\n",
        "                    frame = min(frame, feat_half1.shape[0]-1)\n",
        "                    label_half1[frame][label] = value\n",
        "\n",
        "                if half == 2:\n",
        "                    frame = min(frame, feat_half2.shape[0]-1)\n",
        "                    label_half2[frame][label] = value\n",
        "\n",
        "        \n",
        "            \n",
        "\n",
        "        feat_half1 = feats2clip(torch.from_numpy(feat_half1), \n",
        "                        stride=1, off=int(self.window_size_frame/2), \n",
        "                        clip_length=self.window_size_frame)\n",
        "\n",
        "        feat_half2 = feats2clip(torch.from_numpy(feat_half2), \n",
        "                        stride=1, off=int(self.window_size_frame/2), \n",
        "                        clip_length=self.window_size_frame)\n",
        "\n",
        "        \n",
        "        return self.listGames[index], feat_half1, feat_half2, label_half1, label_half2\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.listGames)\n",
        "\n"
      ],
      "metadata": {
        "id": "YdWQ4GiLuYeb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Defining Loss"
      ],
      "metadata": {
        "id": "fiiUOXn7zdqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "class NLLLoss(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NLLLoss, self).__init__()\n",
        "\n",
        "    def forward(self, labels, output):\n",
        "        # return torch.mean(labels * -torch.log(output) + (1 - labels) * -torch.log(1 - output))\n",
        "\n",
        "        return torch.mean(torch.mean(labels * -torch.log(output) + (1 - labels) * -torch.log(1 - output)))\n"
      ],
      "metadata": {
        "id": "FhhyVILzuqbB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating the Model"
      ],
      "metadata": {
        "id": "hSdqf47Izh_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NetRVLAD(nn.Module):\n",
        "    def __init__(self, cluster_size, feature_size, add_batch_norm=True):\n",
        "        super(NetRVLAD, self).__init__()\n",
        "        self.feature_size = feature_size\n",
        "        self.cluster_size = cluster_size\n",
        "        self.clusters = nn.Parameter((1/math.sqrt(feature_size))\n",
        "                *torch.randn(feature_size, cluster_size))\n",
        "        # self.clusters2 = nn.Parameter((1/math.sqrt(feature_size))\n",
        "        #         *th.randn(1, feature_size, cluster_size))\n",
        "        # self.clusters = nn.Parameter(torch.rand(1,feature_size, cluster_size))\n",
        "        # self.clusters2 = nn.Parameter(torch.rand(1,feature_size, cluster_size))\n",
        "\n",
        "        self.add_batch_norm = add_batch_norm\n",
        "        # self.batch_norm = nn.BatchNorm1d(cluster_size)\n",
        "        self.out_dim = cluster_size*feature_size\n",
        "        #  (+ 128 params?)\n",
        "    def forward(self,x):\n",
        "        max_sample = x.size()[1]\n",
        "\n",
        "        # LOUPE\n",
        "        if self.add_batch_norm: # normalization along feature dimension\n",
        "            x = F.normalize(x, p=2, dim=2)\n",
        "\n",
        "        x = x.reshape(-1,self.feature_size)\n",
        "        assignment = torch.matmul(x,self.clusters)\n",
        "\n",
        "        assignment = F.softmax(assignment,dim=1)\n",
        "        assignment = assignment.view(-1, max_sample, self.cluster_size)\n",
        "\n",
        "        # a_sum = th.sum(assignment,-2,keepdim=True)\n",
        "        # a = a_sum*self.clusters2\n",
        "\n",
        "        assignment = assignment.transpose(1,2)\n",
        "\n",
        "        x = x.view(-1, max_sample, self.feature_size)\n",
        "        rvlad = torch.matmul(assignment, x)\n",
        "        rvlad = rvlad.transpose(-1,1)\n",
        "\n",
        "        # vlad = vlad.transpose(1,2)\n",
        "        # vlad = vlad - a\n",
        "\n",
        "        # L2 intra norm\n",
        "        rvlad = F.normalize(rvlad)\n",
        "        \n",
        "        # flattening + L2 norm\n",
        "        rvlad = rvlad.reshape(-1, self.cluster_size*self.feature_size)\n",
        "        rvlad = F.normalize(rvlad)\n",
        "\n",
        "        return rvlad"
      ],
      "metadata": {
        "id": "n5yMvCavxAyq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the Model"
      ],
      "metadata": {
        "id": "6REa7cD4zxig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, weights=None, input_size=8576, num_classes=17, vocab_size=64, window_size=10, framerate=1, pool=\"NetRVLAD++\"):\n",
        "        \"\"\"\n",
        "        INPUT: a Tensor of shape (batch_size,window_size,feature_size)\n",
        "        OUTPUTS: a Tensor of shape (batch_size,num_classes+1)\n",
        "        \"\"\"\n",
        "\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        self.window_size_frame=window_size * framerate\n",
        "        self.input_size = input_size\n",
        "        self.num_classes = num_classes\n",
        "        self.framerate = framerate\n",
        "        self.pool = pool\n",
        "        self.vlad_k = vocab_size\n",
        "        \n",
        "        # are feature alread PCA'ed?\n",
        "        if not self.input_size == 512:   \n",
        "            self.feature_extractor = nn.Linear(self.input_size, 512)\n",
        "            input_size = 512\n",
        "            self.input_size = 512\n",
        "\n",
        "        if self.pool == \"MAX\":\n",
        "            self.pool_layer = nn.MaxPool1d(self.window_size_frame, stride=1)\n",
        "            self.fc = nn.Linear(input_size, self.num_classes+1)\n",
        "\n",
        "        if self.pool == \"MAX++\":\n",
        "            self.pool_layer_before = nn.MaxPool1d(int(self.window_size_frame/2), stride=1)\n",
        "            self.pool_layer_after = nn.MaxPool1d(int(self.window_size_frame/2), stride=1)\n",
        "            self.fc = nn.Linear(2*input_size, self.num_classes+1)\n",
        "\n",
        "\n",
        "        if self.pool == \"AVG\":\n",
        "            self.pool_layer = nn.AvgPool1d(self.window_size_frame, stride=1)\n",
        "            self.fc = nn.Linear(input_size, self.num_classes+1)\n",
        "\n",
        "        if self.pool == \"AVG++\":\n",
        "            self.pool_layer_before = nn.AvgPool1d(int(self.window_size_frame/2), stride=1)\n",
        "            self.pool_layer_after = nn.AvgPool1d(int(self.window_size_frame/2), stride=1)\n",
        "            self.fc = nn.Linear(2*input_size, self.num_classes+1)\n",
        "\n",
        "\n",
        "        elif self.pool == \"NetVLAD\":\n",
        "            self.pool_layer = NetVLAD(cluster_size=self.vlad_k, feature_size=self.input_size,\n",
        "                                            add_batch_norm=True)\n",
        "            self.fc = nn.Linear(input_size*self.vlad_k, self.num_classes+1)\n",
        "\n",
        "        elif self.pool == \"NetVLAD++\":\n",
        "            self.pool_layer_before = NetVLAD(cluster_size=int(self.vlad_k/2), feature_size=self.input_size,\n",
        "                                            add_batch_norm=True)\n",
        "            self.pool_layer_after = NetVLAD(cluster_size=int(self.vlad_k/2), feature_size=self.input_size,\n",
        "                                            add_batch_norm=True)\n",
        "            self.fc = nn.Linear(input_size*self.vlad_k, self.num_classes+1)\n",
        "\n",
        "\n",
        "\n",
        "        elif self.pool == \"NetRVLAD\":\n",
        "            self.pool_layer = NetRVLAD(cluster_size=self.vlad_k, feature_size=self.input_size,\n",
        "                                            add_batch_norm=True)\n",
        "            self.fc = nn.Linear(input_size*self.vlad_k, self.num_classes+1)\n",
        "\n",
        "        elif self.pool == \"NetRVLAD++\":\n",
        "            self.pool_layer_before = NetRVLAD(cluster_size=int(self.vlad_k/2), feature_size=self.input_size,\n",
        "                                            add_batch_norm=True)\n",
        "            self.pool_layer_after = NetRVLAD(cluster_size=int(self.vlad_k/2), feature_size=self.input_size,\n",
        "                                            add_batch_norm=True)\n",
        "            self.fc = nn.Linear(input_size*self.vlad_k, self.num_classes+1)\n",
        "\n",
        "        self.drop = nn.Dropout(p=0.4)\n",
        "        self.sigm = nn.Sigmoid()\n",
        "\n",
        "        self.load_weights(weights=weights)\n",
        "\n",
        "    def load_weights(self, weights=None):\n",
        "        if(weights is not None):\n",
        "            print(\"=> loading checkpoint '{}'\".format(weights))\n",
        "            checkpoint = torch.load(weights)\n",
        "            self.load_state_dict(checkpoint['state_dict'])\n",
        "            print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
        "                  .format(weights, checkpoint['epoch']))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # input_shape: (batch,frames,dim_features)\n",
        "\n",
        "\n",
        "        BS, FR, IC = inputs.shape\n",
        "        if not IC == 512:\n",
        "            inputs = inputs.reshape(BS*FR, IC)\n",
        "            inputs = self.feature_extractor(inputs)\n",
        "            inputs = inputs.reshape(BS, FR, -1)\n",
        "\n",
        "        # Temporal pooling operation\n",
        "        if self.pool == \"MAX\" or self.pool == \"AVG\":\n",
        "            inputs_pooled = self.pool_layer(inputs.permute((0, 2, 1))).squeeze(-1)\n",
        "\n",
        "        elif self.pool == \"MAX++\" or self.pool == \"AVG++\":\n",
        "            nb_frames_50 = int(inputs.shape[1]/2)    \n",
        "            input_before = inputs[:, :nb_frames_50, :]        \n",
        "            input_after = inputs[:, nb_frames_50:, :]  \n",
        "            inputs_before_pooled = self.pool_layer_before(input_before.permute((0, 2, 1))).squeeze(-1)\n",
        "            inputs_after_pooled = self.pool_layer_after(input_after.permute((0, 2, 1))).squeeze(-1)\n",
        "            inputs_pooled = torch.cat((inputs_before_pooled, inputs_after_pooled), dim=1)\n",
        "\n",
        "\n",
        "        elif self.pool == \"NetVLAD\" or self.pool == \"NetRVLAD\":\n",
        "            inputs_pooled = self.pool_layer(inputs)\n",
        "\n",
        "        elif self.pool == \"NetVLAD++\" or self.pool == \"NetRVLAD++\":\n",
        "            nb_frames_50 = int(inputs.shape[1]/2)\n",
        "            inputs_before_pooled = self.pool_layer_before(inputs[:, :nb_frames_50, :])\n",
        "            inputs_after_pooled = self.pool_layer_after(inputs[:, nb_frames_50:, :])\n",
        "            inputs_pooled = torch.cat((inputs_before_pooled, inputs_after_pooled), dim=1)\n",
        "\n",
        "\n",
        "        # Extra FC layer with dropout and sigmoid activation\n",
        "        output = self.sigm(self.fc(self.drop(inputs_pooled)))\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "BS =256\n",
        "T = 15\n",
        "framerate= 2\n",
        "D = 512\n",
        "pool = \"NetRVLAD++\"\n",
        "model = Model()\n",
        "print(model)\n",
        "inp = torch.rand([BS,T*framerate,D])\n",
        "print(inp.shape)\n",
        "output = model(inp)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sudBuoOXxZi5",
        "outputId": "a19a89c8-8c5f-4c93-9ae8-37fddaefbc7f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model(\n",
            "  (feature_extractor): Linear(in_features=8576, out_features=512, bias=True)\n",
            "  (pool_layer_before): NetRVLAD()\n",
            "  (pool_layer_after): NetRVLAD()\n",
            "  (fc): Linear(in_features=32768, out_features=18, bias=True)\n",
            "  (drop): Dropout(p=0.4, inplace=False)\n",
            "  (sigm): Sigmoid()\n",
            ")\n",
            "torch.Size([256, 30, 512])\n",
            "torch.Size([256, 18])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the Model"
      ],
      "metadata": {
        "id": "YvJlLzahzMQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trainer(train_loader,\n",
        "            val_loader,\n",
        "            val_metric_loader,\n",
        "            model,\n",
        "            optimizer,\n",
        "            #scheduler,\n",
        "            criterion,\n",
        "            #model_name,\n",
        "            max_epochs=10,\n",
        "            evaluation_frequency=2):\n",
        "\n",
        "    logging.info(\"start training\")\n",
        "\n",
        "    best_loss = 9e99\n",
        "\n",
        "    for epoch in range(max_epochs):\n",
        "        #best_model_path = os.path.join(\"models\", model_name, \"model.pth.tar\")\n",
        "\n",
        "        # train for one epoch\n",
        "        loss_training = train(train_loader, model, criterion,\n",
        "                              optimizer, epoch + 1, train=True)\n",
        "\n",
        "        # evaluate on validation set\n",
        "        loss_validation = train(\n",
        "            val_loader, model, criterion, optimizer, epoch + 1, train=False)\n",
        "\n",
        "        #state = {\n",
        "        #    'epoch': epoch + 1,\n",
        "        #    'state_dict': model.state_dict(),\n",
        "        #    'best_loss': best_loss,\n",
        "        #    'optimizer': optimizer.state_dict(),\n",
        "        #}\n",
        "        #os.makedirs(os.path.join(\"models\", model_name), exist_ok=True)\n",
        "\n",
        "        # remember best prec@1 and save checkpoint\n",
        "        #is_better = loss_validation < best_loss\n",
        "        #best_loss = min(loss_validation, best_loss)\n",
        "\n",
        "        # Save the best model based on loss only if the evaluation frequency too long\n",
        "        #if is_better:\n",
        "        #    torch.save(state, best_model_path)\n",
        "\n",
        "        # Test the model on the validation set\n",
        "        if epoch % evaluation_frequency == 0 and epoch != 0:\n",
        "            performance_validation = test(\n",
        "                val_metric_loader,\n",
        "                model)\n",
        "\n",
        "            print(\"Validation performance at epoch \" +\n",
        "                         str(epoch+1) + \" -> \" + str(performance_validation))\n",
        "\n",
        "        # Reduce LR on Plateau after patience reached\n",
        "        #prevLR = optimizer.param_groups[0]['lr']\n",
        "        #scheduler.step(loss_validation)\n",
        "        #currLR = optimizer.param_groups[0]['lr']\n",
        "        #if (currLR is not prevLR and scheduler.num_bad_epochs == 0):\n",
        "        #    logging.info(\"Plateau Reached!\")\n",
        "\n",
        "        #if (prevLR < 2 * scheduler.eps and\n",
        "        #        scheduler.num_bad_epochs >= scheduler.patience):\n",
        "        #    logging.info(\n",
        "        #        \"Plateau Reached and no more reduction -> Exiting Loop\")\n",
        "        #    break\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "def train(dataloader,\n",
        "          model,\n",
        "          criterion,\n",
        "          optimizer,\n",
        "          epoch,\n",
        "          train=False):\n",
        "\n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "\n",
        "    # switch to train mode\n",
        "    if train:\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "\n",
        "    end = time.time()\n",
        "    with tqdm(enumerate(dataloader), total=len(dataloader)) as t:\n",
        "        for i, (feats, labels) in t:\n",
        "            # measure data loading time\n",
        "            data_time.update(time.time() - end)\n",
        "            feats = feats.cuda()\n",
        "            labels = labels.cuda()\n",
        "            # compute output\n",
        "            output = model(feats)\n",
        "\n",
        "            # hand written NLL criterion\n",
        "            loss = criterion(labels, output)\n",
        "\n",
        "            # measure accuracy and record loss\n",
        "            losses.update(loss.item(), feats.size(0))\n",
        "\n",
        "            if train:\n",
        "                # compute gradient and do SGD step\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            # measure elapsed time\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "\n",
        "            if train:\n",
        "                desc = f'Train {epoch}: '\n",
        "            else:\n",
        "                desc = f'Evaluate {epoch}: '\n",
        "            desc += f'Time {batch_time.avg:.3f}s '\n",
        "            desc += f'(it:{batch_time.val:.3f}s) '\n",
        "            desc += f'Data:{data_time.avg:.3f}s '\n",
        "            desc += f'(it:{data_time.val:.3f}s) '\n",
        "            desc += f'Loss {losses.avg:.4e} '\n",
        "            t.set_description(desc)\n",
        "\n",
        "    return losses.avg\n",
        "\n",
        "def test(dataloader, model):\n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    end = time.time()\n",
        "    all_labels = []\n",
        "    all_outputs = []\n",
        "    with tqdm(enumerate(dataloader), total=len(dataloader)) as t:\n",
        "        for i, (feats, labels) in t:\n",
        "            # measure data loading time\n",
        "            data_time.update(time.time() - end)\n",
        "            feats = feats.cuda()\n",
        "            # labels = labels.cuda()\n",
        "\n",
        "            # print(feats.shape)\n",
        "            # feats=feats.unsqueeze(0)\n",
        "            # print(feats.shape)\n",
        "\n",
        "            # compute output\n",
        "            output = model(feats)\n",
        "\n",
        "            all_labels.append(labels.detach().numpy())\n",
        "            all_outputs.append(output.cpu().detach().numpy())\n",
        "\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "\n",
        "            desc = f'Test (cls): '\n",
        "            desc += f'Time {batch_time.avg:.3f}s '\n",
        "            desc += f'(it:{batch_time.val:.3f}s) '\n",
        "            desc += f'Data:{data_time.avg:.3f}s '\n",
        "            desc += f'(it:{data_time.val:.3f}s) '\n",
        "            t.set_description(desc)\n",
        "\n",
        "    AP = []\n",
        "    for i in range(1, dataloader.dataset.num_classes+1):\n",
        "        AP.append(average_precision_score(np.concatenate(all_labels)\n",
        "                                          [:, i], np.concatenate(all_outputs)[:, i]))\n",
        "\n",
        "    # t.set_description()\n",
        "    # print(AP)\n",
        "    mAP = np.mean(AP)\n",
        "    print(mAP, AP)\n",
        "\n",
        "    return mAP"
      ],
      "metadata": {
        "id": "2fptpWk_z4Lp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Executing the Model"
      ],
      "metadata": {
        "id": "mxRfDvyp0FET"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "soccernet_path = \"drive/MyDrive/SoccerNetDatasetFeatures/\"\n",
        "\n",
        "dataset_Train = SoccerNetClips(path=soccernet_path)\n",
        "dataset_Valid = SoccerNetClips(path=soccernet_path,  split=[\"valid\"])\n",
        "dataset_Valid_metric  = SoccerNetClips(path=soccernet_path, split=[\"valid\"])\n",
        "\n",
        "\n",
        "model = Model().cuda()\n",
        "logging.info(model)\n",
        "total_params = sum(p.numel()\n",
        "                       for p in model.parameters() if p.requires_grad)\n",
        "logging.info(\"Total number of parameters: \" + str(total_params))\n",
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset_Train,\n",
        "            batch_size=64, shuffle=True,\n",
        "            num_workers=4, pin_memory=True)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(dataset_Valid,\n",
        "            batch_size=64, shuffle=False,\n",
        "            num_workers=4, pin_memory=True)\n",
        "\n",
        "val_metric_loader = torch.utils.data.DataLoader(dataset_Valid_metric,\n",
        "            batch_size=64, shuffle=False,\n",
        "            num_workers=4, pin_memory=True)\n",
        "\n",
        "criterion = NLLLoss().cuda()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5, \n",
        "                                    betas=(0.9, 0.999), eps=1e-08, \n",
        "                                    weight_decay=0, amsgrad=False)\n",
        "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', verbose=True)\n",
        "trainer(train_loader, val_loader, val_metric_loader, model, optimizer, criterion)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72xz5_Z6z9kj",
        "outputId": "b66d2900-5be3-4716-fe9b-6390d37a9bc4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [00:37<00:00,  1.32it/s]\n",
            "100%|██████████| 50/50 [00:29<00:00,  1.70it/s]\n",
            "100%|██████████| 50/50 [00:22<00:00,  2.21it/s]\n",
            "Train 1: Time 0.022s (it:0.011s) Data:0.008s (it:0.003s) Loss 4.9423e-01 : 100%|██████████| 434/434 [00:08<00:00, 51.57it/s]\n",
            "Evaluate 1: Time 0.018s (it:0.010s) Data:0.013s (it:0.006s) Loss 3.5157e-01 : 100%|██████████| 430/430 [00:06<00:00, 65.97it/s]\n",
            "Train 2: Time 0.021s (it:0.008s) Data:0.007s (it:0.001s) Loss 2.8207e-01 : 100%|██████████| 434/434 [00:08<00:00, 53.15it/s]\n",
            "Evaluate 2: Time 0.018s (it:0.009s) Data:0.013s (it:0.006s) Loss 2.2924e-01 : 100%|██████████| 430/430 [00:06<00:00, 65.15it/s]\n",
            "Train 3: Time 0.021s (it:0.009s) Data:0.007s (it:0.001s) Loss 2.0182e-01 : 100%|██████████| 434/434 [00:08<00:00, 53.31it/s]\n",
            "Evaluate 3: Time 0.018s (it:0.010s) Data:0.013s (it:0.007s) Loss 1.7836e-01 : 100%|██████████| 430/430 [00:06<00:00, 64.73it/s]\n",
            "Test (cls): Time 0.026s (it:0.009s) Data:0.013s (it:0.006s) : 100%|██████████| 430/430 [00:10<00:00, 42.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.12831096784964338 [0.14641986031707532, 0.016188682089266423, 0.0898654572815745, 0.020970071480460168, 0.016746353490021893, 0.12566975088350002, 0.16556310129040708, 0.04943329645341987, 0.608699086701906, 0.4115862205791728, 0.08107597310314314, 0.0471292884375216, 0.08903546748040077, 0.2868891619362755, 0.02534792267892492, 0.0004494247088141252, 0.00021733453205330483]\n",
            "Validation performance at epoch 3 -> 0.12831096784964338\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Train 4: Time 0.021s (it:0.008s) Data:0.007s (it:0.001s) Loss 1.6593e-01 : 100%|██████████| 434/434 [00:08<00:00, 53.07it/s]\n",
            "Evaluate 4: Time 0.018s (it:0.010s) Data:0.012s (it:0.007s) Loss 1.5316e-01 : 100%|██████████| 430/430 [00:06<00:00, 64.35it/s]\n",
            "Train 5: Time 0.021s (it:0.009s) Data:0.007s (it:0.001s) Loss 1.4522e-01 : 100%|██████████| 434/434 [00:08<00:00, 52.71it/s]\n",
            "Evaluate 5: Time 0.018s (it:0.010s) Data:0.012s (it:0.006s) Loss 1.3519e-01 : 100%|██████████| 430/430 [00:06<00:00, 64.97it/s]\n",
            "Test (cls): Time 0.018s (it:0.011s) Data:0.013s (it:0.007s) : 100%|██████████| 430/430 [00:06<00:00, 64.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.11473765302152755 [0.18957168906929167, 0.012343942553247955, 0.15569785087069377, 0.016486632128230344, 0.019549075263905816, 0.1467941010137189, 0.1794340843774903, 0.03882775363260976, 0.5281109260665895, 0.258028116920993, 0.07337056542131452, 0.03646675868056548, 0.11218219254343774, 0.16899589509973822, 0.014184044510485248, 0.0003746192748344654, 0.00012185393882163392]\n",
            "Validation performance at epoch 5 -> 0.11473765302152755\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Train 6: Time 0.021s (it:0.009s) Data:0.007s (it:0.001s) Loss 1.2893e-01 : 100%|██████████| 434/434 [00:08<00:00, 52.54it/s]\n",
            "Evaluate 6: Time 0.018s (it:0.010s) Data:0.012s (it:0.006s) Loss 1.2109e-01 : 100%|██████████| 430/430 [00:06<00:00, 63.63it/s]\n",
            "Train 7: Time 0.021s (it:0.009s) Data:0.007s (it:0.002s) Loss 1.1648e-01 : 100%|██████████| 434/434 [00:08<00:00, 53.31it/s]\n",
            "Evaluate 7: Time 0.018s (it:0.011s) Data:0.012s (it:0.007s) Loss 1.1077e-01 : 100%|██████████| 430/430 [00:06<00:00, 64.08it/s]\n",
            "Test (cls): Time 0.018s (it:0.010s) Data:0.013s (it:0.006s) : 100%|██████████| 430/430 [00:06<00:00, 64.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.11563085227170294 [0.27484114810533256, 0.017985278607480726, 0.17741658213160208, 0.02589640009887169, 0.020536788448312686, 0.13112600658380563, 0.13985208213247916, 0.04750437512770179, 0.5027884102962255, 0.24950720646529462, 0.09677578082045077, 0.045686875580536405, 0.09427575867562443, 0.1172964070537906, 0.02328975653804538, 0.0008189439835268879, 0.00012668796986902377]\n",
            "Validation performance at epoch 7 -> 0.11563085227170294\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Train 8: Time 0.021s (it:0.010s) Data:0.007s (it:0.003s) Loss 1.0742e-01 : 100%|██████████| 434/434 [00:08<00:00, 52.76it/s]\n",
            "Evaluate 8: Time 0.018s (it:0.010s) Data:0.012s (it:0.006s) Loss 1.0340e-01 : 100%|██████████| 430/430 [00:06<00:00, 64.21it/s]\n",
            "Train 9: Time 0.021s (it:0.008s) Data:0.007s (it:0.001s) Loss 1.0082e-01 : 100%|██████████| 434/434 [00:08<00:00, 52.95it/s]\n",
            "Evaluate 9: Time 0.018s (it:0.010s) Data:0.012s (it:0.006s) Loss 9.8059e-02 : 100%|██████████| 430/430 [00:06<00:00, 64.28it/s]\n",
            "Test (cls): Time 0.018s (it:0.011s) Data:0.013s (it:0.008s) : 100%|██████████| 430/430 [00:06<00:00, 63.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.11955290569638144 [0.24436854201120714, 0.017561968281903352, 0.1822926087404041, 0.02184269469500308, 0.021444864957994175, 0.13567074377026234, 0.14731715639873516, 0.051013925645895276, 0.5446113679401267, 0.25992888853013824, 0.10908629109238667, 0.05272532450841179, 0.09099953970047418, 0.1275007656846865, 0.02443949180998853, 0.0014469854858267645, 0.00014823758504043446]\n",
            "Validation performance at epoch 9 -> 0.11955290569638144\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Train 10: Time 0.021s (it:0.009s) Data:0.007s (it:0.002s) Loss 9.5811e-02 : 100%|██████████| 434/434 [00:08<00:00, 52.85it/s]\n",
            "Evaluate 10: Time 0.018s (it:0.011s) Data:0.012s (it:0.006s) Loss 9.4129e-02 : 100%|██████████| 430/430 [00:06<00:00, 64.49it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.cuda.is_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mke0FXz9A9Z-",
        "outputId": "74af1fbb-3090-4cc2-82a5-3dee6354f97f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7Mun0aUJD9nd"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}